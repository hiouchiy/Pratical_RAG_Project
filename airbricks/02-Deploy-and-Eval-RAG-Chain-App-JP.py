# Databricks notebook source
# MAGIC %md 
# MAGIC ### ç’°å¢ƒ
# MAGIC - ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—: ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC # RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆï¼ãƒ‡ãƒ—ãƒ­ã‚¤ï¼è©•ä¾¡
# MAGIC
# MAGIC å‰å›ã®[01-Data-Preparation-and-Index]($./01-Data-Preparation-and-Index [DO NOT EDIT])ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã€RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å¿…è¦ãªä»¥ä¸‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æº–å‚™ã—ã¾ã—ãŸã€‚
# MAGIC - FAQãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (Embeddingãƒ¢ãƒ‡ãƒ«å«ã‚€)
# MAGIC - æ–‡ç« ç”ŸæˆLLMï¼ˆDBRXï¼‰ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# MAGIC
# MAGIC ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Mosaic AI Agent Frameworkã‚’ä½¿ç”¨ã—ã¦ã€ã“ã‚Œã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç¹‹ãåˆã‚ã›ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«é©åˆ‡ã«å›ç­”ã™ã‚‹ RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ã‚¢ãƒ—ãƒªï¼ˆãƒã‚§ãƒ¼ãƒ³ã‚„ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’ä½œæˆã—ã€ãã‚Œã‚’ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚
# MAGIC
# MAGIC ã•ã‚‰ã«ã€Mosaic AI Agent Evaluationã‚’ä½¿ç”¨ã—ã¦ã€RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ã‚¢ãƒ—ãƒªã®è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

# COMMAND ----------

# MAGIC %pip install --quiet -U databricks-agents==0.1.0 mlflow-skinny==2.14.0 mlflow==2.14.0 mlflow[gateway] langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4 databricks-vectorsearch==0.38 databricks-sdk==0.23.0 openai
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,ã‚³ãƒ³ãƒ•ã‚£ã‚°(ç’°å¢ƒã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ï¼‰
# MAGIC %run ./config

# COMMAND ----------

# MAGIC %md
# MAGIC ### RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ã‚¢ãƒ—ãƒªã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’YAMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜

# COMMAND ----------

import yaml
import mlflow

rag_chain_config = {
      "vector_search_endpoint_name": VECTOR_SEARCH_ENDPOINT_NAME,
      "vector_search_index_name": f"{catalog}.{dbName}.{embed_table_name}_vs_index",
      "llm_endpoint_name": instruct_endpoint_name,
}
config_file_name = 'rag_chain_config.yaml'
try:
    with open(config_file_name, 'w') as f:
        yaml.dump(rag_chain_config, f)
except:
    print('pass to work on build job')

# COMMAND ----------

import os

API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()
os.environ["DATABRICKS_HOST"] = API_ROOT
API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
os.environ["DATABRICKS_TOKEN"] = API_TOKEN

# COMMAND ----------

# MAGIC %md
# MAGIC ### RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè£…ã™ã‚‹
# MAGIC
# MAGIC ã“ã®ãƒ‡ãƒ¢ã§ã¯pyfunc.PythonModelãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…ã‚’ã—ã¾ã™ã€‚
# MAGIC
# MAGIC å®Ÿè£…ã€ãŠã‚ˆã³å‹•ä½œç¢ºèªå¾Œã€ã“ã®ã‚»ãƒ«ã®ã‚³ãƒ¼ãƒ‰ã‚’ä»¥ä¸‹ã®ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦.pyãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ›¸ãå‡ºã—ã¾ã™ã€‚
# MAGIC
# MAGIC %%writefile chain.py

# COMMAND ----------

# %%writefile chain.py
import os

import pandas as pd

import mlflow
import mlflow.deployments

from databricks.vector_search.client import VectorSearchClient
from langchain_core.prompts.chat import HumanMessagePromptTemplate
from openai import OpenAI

class AirbricksRAGAgentApp(mlflow.pyfunc.PythonModel):

    def __init__(self):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        """

        self.model_config = mlflow.models.ModelConfig(development_config="rag_chain_config.yaml")

        try:
            # ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ›ã‚¹ãƒˆã«"DB_MODEL_SERVING_HOST_URL"ãŒè‡ªå‹•è¨­å®šã•ã‚Œã‚‹ã®ã§ã€ãã®å†…å®¹ã‚’DATABRICKS_HOSTã«ã‚‚è¨­å®š
            os.environ["DATABRICKS_HOST"] = os.environ["DB_MODEL_SERVING_HOST_URL"]
        except:
            pass

        vsc = VectorSearchClient(disable_notice=True)
        self.vs_index = vsc.get_index(
            endpoint_name=self.model_config.get("vector_search_endpoint_name"),
            index_name=self.model_config.get("vector_search_index_name")
        )

        # ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¢ã‚¯ã‚»ã‚¹ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—
        self.deploy_client = mlflow.deployments.get_deploy_client("databricks")

        # LLMåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
        self.chat_model = OpenAI(
            api_key=os.environ.get("DATABRICKS_TOKEN"),
            base_url=os.environ.get("DATABRICKS_HOST") + "/serving-endpoints",
        )

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
        self.SYSTEM_MESSAGE = "ã€å‚è€ƒæƒ…å ±ã€‘ã®ã¿ã‚’å‚è€ƒã«ã—ãªãŒã‚‰ã€è³ªå•ã€‘ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ã‚ã‹ã‚‰ãªã„å ´åˆã‚„ã€è³ªå•ãŒé©åˆ‡ã§ãªã„å ´åˆã¯ã€åˆ†ã‹ã‚‰ãªã„æ—¨ã‚’ç­”ãˆã¦ãã ã•ã„ã€‚ã€å‚è€ƒæƒ…å ±ã€‘ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„äº‹å®Ÿã‚’ç­”ãˆã‚‹ã®ã¯ã‚„ã‚ã¦ãã ã•ã„ã€‚"

        # ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æº–å‚™
        human_template = """ã€å‚è€ƒæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}"""
        self.HUMAN_MESSAGE = HumanMessagePromptTemplate.from_template(human_template)

        
    def _find_relevant_doc(self, question, num_results = 10, relevant_threshold = 0.7):
        """
        ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã€é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢
        """

        results = self.vs_index.similarity_search(
            query_text=question,
            columns=["query", "response"],
            num_results=num_results)
        
        docs = results.get('result', {}).get('data_array', [])

        #é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚0.7ä»¥ä¸‹ã¯ã€é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹
        returned_docs = []
        for doc in docs:
          if doc[-1] > relevant_threshold:
            returned_docs.append({"query": doc[0], "response": doc[1]})

        return returned_docs
    

    def _build_prompt(self, docs, question):
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        """

        context = ""
        for doc in docs:
          context = context + doc['response'] + "\n\n"

        human_message = self.HUMAN_MESSAGE.format_messages(
          context=context, 
          question=question
        )
    
        prompt=[
            {
                "role": "system",
                "content": self.SYSTEM_MESSAGE
            },
            {
                "role": "user",
                "content": human_message[0].content,
            }
        ]

        return prompt


    @mlflow.trace(name="predict_rag")
    def predict(self, context, model_input, params=None):
        """
        æ¨è«–ãƒ¡ã‚¤ãƒ³é–¢æ•°
        """

        if isinstance(model_input, pd.DataFrame):
            model_input = model_input.to_dict(orient="records")[0]

        # FAQãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚’ç”¨ã„ã¦è³ªå•ã¨é¡ä¼¼ã—ã¦ã„ã‚‹æƒ…å ±ã‚’æ¤œç´¢
        with mlflow.start_span(name="_find_relevant_doc") as span:
            question = model_input["messages"][-1]["content"]
            docs = self._find_relevant_doc(question)
            span.set_inputs({"question": question})
            span.set_outputs({"docs": docs})

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        with mlflow.start_span(name="_build_prompt") as span:
            prompt = self._build_prompt(docs, question)
            span.set_inputs({"question": question, "docs": docs})
            span.set_outputs({"prompt": prompt})

        # LLMã«å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹
        with mlflow.start_span(name="generate_answer") as span:
            response = self.chat_model.chat.completions.create(
                model=self.model_config.get("llm_endpoint_name"),
                messages=prompt,
                max_tokens=2000,
                temperature=0.1
            )
            span.set_inputs({"question": question, "prompt": prompt})
            span.set_outputs({"answer": response})
        
        
        # å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ã—ã¦è¿”ã™.
        # ChatCompletionResponseã®å½¢å¼ã§è¿”ã•ãªã„ã¨å¾Œã€…ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã€‚
        return response.to_dict()


mlflow.models.set_model(model=AirbricksRAGAgentApp())

# COMMAND ----------

# MAGIC %md
# MAGIC ### RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹

# COMMAND ----------

input_example = {
  "messages": [{"role": "user", "content": "æ–°ã—ã„ã‚¨ã‚¢ã‚³ãƒ³ã‚’é¸ã¶éš›ã«æœ€ã‚‚é‡è¦ãªã“ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"}]
}

rag_model = AirbricksRAGAgentApp()
rag_model.predict(None, model_input=input_example)

# COMMAND ----------

# MAGIC %md
# MAGIC ### RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ã‚¢ãƒ—ãƒªã‚’Unity Catalogã«ç™»éŒ²

# COMMAND ----------

import os

# Specify the full path to the chain notebook
chain_notebook_path = os.path.join(os.getcwd(), "chain.py")

# Specify the full path to the config file (.yaml)
config_file_path = os.path.join(os.getcwd(), config_file_name)

print(f"Chain notebook path: {chain_notebook_path}")
print(f"Chain notebook path: {config_file_path}")

# COMMAND ----------

import numpy as np
import pandas as pd

import mlflow
from mlflow.models import infer_signature

with mlflow.start_run(run_name="airbricks_rag_chatbot"):
  # å…¥å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
  input_example = {
    "messages": [
        {
            "role": "user",
            "content": "æ–°ã—ã„ã‚¨ã‚¢ã‚³ãƒ³ã‚’é¸ã¶éš›ã«æœ€ã‚‚é‡è¦ãªã“ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        }
    ],
  }

  output_response = {
    'id': 'chatcmpl_e048d1af-4b9c-4cc9-941f-0311ac5aa7ab',
    'choices': [
      {
        'finish_reason': 'stop', 
        'index': 0,
        'logprobs': "",
        'message': {
          'content': 'æ–°ã—ã„ã‚¨ã‚¢ã‚³ãƒ³ã‚’é¸ã¶éš›ã«æœ€ã‚‚é‡è¦ãªã“ã¨ã¯ã€å†·å´èƒ½åŠ›ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã€ã‚µã‚¤ã‚ºã€ç‰¹å®šã®æ©Ÿèƒ½ï¼ˆä¾‹ãˆã°ç©ºæ°—æµ„åŒ–ã‚„Wi-Fiæ¥ç¶šï¼‰ãªã©ã€ã”è‡ªå®…ã‚„ã‚ªãƒ•ã‚£ã‚¹ã®ãƒ‹ãƒ¼ã‚ºã«æœ€é©ãªç‰¹æ€§ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã™ã€‚',
          'role': 'assistant'
          }
        }
      ],
    'created': 1719722525,
    'model': 'dbrx-instruct-032724',
    'object': 'chat.completion',
    'usage': {'completion_tokens': 279,'prompt_tokens': 1386,'total_tokens': 1665}
  }

  signature = infer_signature(
    model_input=input_example, 
    model_output=output_response)
  
  logged_chain_info = mlflow.pyfunc.log_model(
    python_model=chain_notebook_path,
    model_config=config_file_path,
    artifact_path="chain",
    signature=signature, 
    input_example=input_example,
    example_no_conversion=True,
    pip_requirements=[
      "databricks-agents==0.1.0",
      "langchain==0.2.1",
      "langchain_core==0.2.5",
      "langchain_community==0.2.4",
      "databricks-vectorsearch==0.38",
      "databricks-sdk==0.23.0",
      "openai"]
  )

# COMMAND ----------

# Test the chain locally
chain = mlflow.pyfunc.load_model(logged_chain_info.model_uri)
chain.predict(input_example)

# COMMAND ----------

import mlflow

mlflow.set_registry_uri("databricks-uc")

model_name = f"{catalog}.{dbName}.{registered_model_name}"
uc_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=model_name)

# COMMAND ----------

### Test the registered model
registered_agent = mlflow.pyfunc.load_model(f"models:/{model_name}/{uc_model_info.version}")

registered_agent.predict(
  {"messages": [{"role": "user", "content": "æ–°ã—ã„ã‚¨ã‚¢ã‚³ãƒ³ã‚’é¸ã¶éš›ã«æœ€ã‚‚é‡è¦ãªã“ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"}]}
)

# COMMAND ----------

# MAGIC %md 
# MAGIC ### RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ã‚¢ãƒ—ãƒªã‚’ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ»ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤
# MAGIC
# MAGIC æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ã‚¢ãƒ—ãƒªã‚’ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã“ã¨ã§ã™ã€‚
# MAGIC Databricks Agentãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
# MAGIC
# MAGIC
# MAGIC ã“ã‚Œã§ã€ãƒã‚§ãƒ¼ãƒ³ã®è©•ä¾¡ã€ãŠã‚ˆã³ã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚¢ãƒ—ãƒªã‹ã‚‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

# COMMAND ----------

from databricks import agents
deployment_info = agents.deploy(
    model_name, 
    uc_model_info.version, 
    environment_vars={
        "DATABRICKS_TOKEN": "{{secrets/"+databricks_token_secrets_scope+"/"+databricks_token_secrets_key+"}}"
    })

review_instructions = """### æ ªå¼ä¼šç¤¾ã‚¨ã‚¢ãƒ–ãƒªãƒƒã‚¯ã‚¹ FAQãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ†ã‚¹ãƒˆæ‰‹é †

ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å“è³ªå‘ä¸Šã®ãŸã‚ã«ãœã²ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ãã ã•ã„ã€‚

1. **å¤šæ§˜ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„**ï¼š
   - å®Ÿéš›ã®ãŠå®¢æ§˜ãŒå°‹ã­ã‚‹ã¨äºˆæƒ³ã•ã‚Œã‚‹å¤šæ§˜ãªè³ªå•ã‚’å…¥åŠ›ãã ã•ã„ã€‚ã“ã‚Œã¯ã€äºˆæƒ³ã•ã‚Œã‚‹è³ªå•ã‚’åŠ¹æœçš„ã«å‡¦ç†ã§ãã‚‹ã‹å¦ã‹ã‚’ç¢ºèªã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

2. **å›ç­”ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**ï¼š
   - è³ªå•ã®å¾Œã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å›ç­”ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
   - å›ç­”ãŒé–“é•ã£ã¦ã„ãŸã‚Šã€æ”¹å–„ã™ã¹ãç‚¹ãŒã‚ã‚‹å ´åˆã¯ã€ã€Œå›ç­”ã®ç·¨é›†ï¼ˆEdit Responseï¼‰ã€ã§ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚çš†æ§˜ã®ä¿®æ­£ã«ã‚ˆã‚Šã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç²¾åº¦ã‚’å‘ä¸Šã§ãã¾ã™ã€‚

3. **å›ç­”ã«ä»˜éšã—ã¦ã„ã‚‹å‚è€ƒæ–‡çŒ®ã®ç¢ºèª**ï¼š
   - è³ªå•ã«å¯¾ã—ã¦ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å›ç­”ã•ã‚Œã‚‹å„å‚è€ƒæ–‡çŒ®ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
   - GoodğŸ‘ï¼BadğŸ‘æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€ãã®æ–‡æ›¸ãŒè³ªå•å†…å®¹ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ãã ã•ã„ã€‚

ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®è©•ä¾¡ã«ãŠæ™‚é–“ã‚’å‰²ã„ã¦ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã‚¨ãƒ³ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é«˜å“è³ªã®è£½å“ã‚’ãŠå±Šã‘ã™ã‚‹ãŸã‚ã«ã¯ã€çš†æ§˜ã®ã”å”åŠ›ãŒä¸å¯æ¬ ã§ã™ã€‚"""

agents.set_review_instructions(model_name, review_instructions)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Mosaic AI Agent Evaluationã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ—ãƒªã‚’ä½¿ç”¨ã—ã¦äººæ‰‹ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œã†
# MAGIC é–¢ä¿‚è€…ã«Mosaic AI Agent Evaluation ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ—ãƒª ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ã‚’ä¸ãˆã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œã£ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ã€‚
# MAGIC ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€é–¢ä¿‚è€…ã¯Databricksã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æŒã£ã¦ã„ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

# COMMAND ----------

from databricks import agents

user_list = ["someone@databricks.com"]
agents.set_permissions(model_name=model_name, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)

print(f"Share this URL with your stakeholders: {deployment_info.review_app_url}")

# COMMAND ----------

# MAGIC %md
# MAGIC ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ‡ãƒ—ãƒ­ã‚¤çŠ¶æ³ã¯[Serving Endpoint UI](#/mlflow/endpoints)ã§ç¢ºèªã§ãã¾ã™ã€‚ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ã¾ã§æ•°åˆ†ç¨‹åº¦è¦ã—ã¾ã™ã€‚
# MAGIC ãªãŠã€Feedbackã¨ã„ã†ã®ãŒã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ—ãƒªç”¨ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚
# MAGIC
# MAGIC ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†å¾Œã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ—ãƒªã®URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦äººæ‰‹ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ### Mosaic AI Agent Evaluation "LLM-as-a-judge" ã‚’ä½¿ç”¨ã—ã¦RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è‡ªå‹•è©•ä¾¡ã‚’è¡Œã†

# COMMAND ----------

import mlflow
import pandas as pd
eval_set  = [
    {
      "request_id": "1",
      "request": "Zenith ZR-450ã®ã‚¿ãƒƒãƒã‚¹ã‚¯ãƒªãƒ¼ãƒ³æ“ä½œãƒ‘ãƒãƒ«ã®åå¿œãŒéˆã„ã§ã™ã€‚ã©ã†ã—ãŸã‚‰è‰¯ã„ã§ã™ã‹ï¼Ÿ",
    },
    {
      "request_id": "2",
      "request": "ã‚¨ã‚¢ã‚³ãƒ³ã‚’è²·ã„æ›ãˆã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®åˆ¤æ–­åŸºæº–ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    },
    {
      "request_id": "3",
      "request": "ãƒªãƒ“ãƒ³ã‚°ãŒï¼“ï¼å¹³ç±³ãªã®ã§ã™ãŒã€ã©ã®è£½å“ãŒãƒ™ã‚¹ãƒˆï¼Ÿ",
    }
]
#### Convert dictionary to a pandas DataFrame
eval_set_df = pd.DataFrame(eval_set)


model_name = f"{catalog}.{dbName}.{registered_model_name}"

###
# mlflow.evaluate() call
###
evaluation_results = mlflow.evaluate(
    data=eval_set_df,
    model=f"models:/{model_name}/{uc_model_info.version}",
    model_type="databricks-agent",
)

# COMMAND ----------

evaluation_results.tables["eval_results"]

# COMMAND ----------

# MAGIC %md
# MAGIC ### æœ€å¾Œã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸRAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«RESTã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã¿ã¾ã—ã‚‡ã†

# COMMAND ----------

import requests
import json

data = {
  "messages": [{"role": "user", "content": "ã‚¨ã‚¢ã‚³ãƒ³ã‚’è²·ã„æ›ãˆã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®åˆ¤æ–­åŸºæº–ã¯ä½•ã§ã™ã‹ï¼Ÿ"}]
}

databricks_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
headers = {"Context-Type": "text/json", "Authorization": f"Bearer {databricks_token}"}

response = requests.post(
    url=f"{API_ROOT}/serving-endpoints/{deployment_info.endpoint_name}/invocations", json=data, headers=headers
)

print(response.json()["choices"][0]["message"]["content"])

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä»¥ä¸Šã§ã™ã€‚
# MAGIC
# MAGIC ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾ã™ã‚‹ãŸã‚ã«ã€æœ¬ã‚µãƒ³ãƒ—ãƒ«ã§ä½œæˆã—ãŸã™ã¹ã¦ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ãã ã•ã„ã€‚
# MAGIC
